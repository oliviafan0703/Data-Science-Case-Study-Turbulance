---
title: "Case Study Report"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
---

```{r setup, message=F, warning=F, echo=F}
require(rstan)
require(tidyverse)
require(rstanarm)
require(magrittr)
library(corrplot)
library(glmnet)
require(CRAN)
library(knitr)
library(caret)
```

```{r load-data, message = FALSE}
data_train<-read_csv("data/data-train.csv")
data_test<-read_csv("data/data-test.csv")
```

## Introduction


## Methodology

### EDA 

After loading the data, we performed exploratory data analysis on all three predictors and four moments.

We first noted that the predictor variables `Re` is clustered at fixed values, with `Re` clustering at 90, 224 and 398 (3 levels).

```{r}
summary(data_train$St)
summary(data_train$Re)
summary(data_train$Fr)
```

We found that the moments are not only highly correlated,

```{r}
res<-cor(data_train)
res
corrplot(res, tl.col ="black")
```

but that they are linearly correlated:

```{r}
pairs(data_train[4:7], cex = 0.5, pch = 19)
```
Therefore, we decided to fit a model on `R_moment_1`, which will give us the relationship of the predictor variables on the other moments due to the high linear correlation between the moments.

Moreover, we noticed that the gravitational acceleration has infinite values, which is problematic. Therefore, we used inverse logit transform on `Fr` to transform the infinity value into a finite value (Inf transformed to 1):

```{r}
data_train <- data_train %>%
  mutate(Re_category = case_when(Re == 90 ~ "Low", Re==224 ~ "Medium", Re == 398 ~ "High"))%>%
  mutate(Fr_transformed = invlogit(Fr))
```

We then explored shrinkage methods such as ridge regression and lasso. However, since we know that the three predictors are all active so that we do not need predictor selection, so we attempted to fit a ridge regression model: 


### Ridge Model

```{r}
y <- data_train$R_moment_1
x <- data.matrix(data_train[, c('Re', 'St', 'Fr_transformed')])
```

```{r}
set.seed(123)
sample <- sample(c(TRUE, FALSE), nrow(data_train), replace=TRUE)
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.test <- y[test]
```

```{r}

lambda_seq = 10^seq(10, -2, length = 100)

ridgemodel <- glmnet(x[train,], y[train], alpha = 0, lambda = lambda_seq)

summary(ridgemodel)
```

```{r}
set.seed(123)
#perform k-fold cross-validation to find optimal lambda value
cv_ridgemodel <- cv.glmnet(x[train,], y[train], alpha = 0)
plot(cv_ridgemodel)
```

```{r}
set.seed(123)
#find optimal lambda value that minimizes test MSE
best_lambda <- cv_ridgemodel$lambda.min
best_lambda
```

```{r}
ridge.pred <- predict(ridgemodel, s = best_lambda, newx = x[test,])
mse <- mean((ridge.pred - y.test)^2)
mse
```

The ridge regression we fitted through cross-validation gives us an MSE of 0.001498519

```{r}
lm.fit_M1 <- lm(R_moment_1 ~ St+ Re_category+Fr_transformed+Fr_transformed*Re_category+St*Re_category, data = data_train)
lm_summary_M1<-summary(lm.fit_M1)
lm_mse <-mean(lm_summary_M1$residual^2)
final_model_M1<-lm.fit_M1
lm_mse
```

By fitting a simple linear regression and adding interaction terms, we obtained a model that has a MSE of 7.656106e-05, which is much smaller than the ridge regression MSE. Moreover, the model fits the data closely with $R^2$ value of 0.9727. Therefore, we decided to use this model as our final model:

```{r}
library(jtools) # Load jtools
summ(final_model_M1)
```

We fitted models on the same model to predict second, third and fourth moments due to the collinearity as explained above:

```{r}
data_train <- data_train %>%
  mutate(Fr_category = case_when(Fr == 0.052 ~ "Low", Fr == 0.3 ~ "Medium", Fr == Inf ~ "High"))

final_model_M2 <- lm(R_moment_2 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train)
final_model_M3 <- lm(R_moment_2 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train)
final_model_M4 <- lm(R_moment_2 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train)
summ(final_model_M2)
summ(final_model_M3)
summ(final_model_M4)
```

Here we perform a 5-fold cross validation on the model.
```{r}
train.control <- trainControl(method = "cv", number = 5)

M1_cv <- train(R_moment_1 ~ St+ Re_category+Fr_transformed+Fr_transformed*Re_category+St*Re_category, data = data_train, method = "lm", trControl = train.control)
print(M1_cv)
```

```{r, warning=FALSE}
M2_cv <- train(R_moment_2 ~ St+Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train, method = "lm", trControl = train.control)
M3_cv <- train(R_moment_3 ~ St+Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train, method = "lm", trControl = train.control)
M4_cv <- train(R_moment_4 ~ St+Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train, method = "lm", trControl = train.control)
print(M2_cv)
print(M3_cv)
print(M4_cv)
```
```{r}
library(knitr)
M1_metrics <- M1_cv$results %>% select(c("RMSE", "Rsquared", "MAE"))
M2_metrics <- M2_cv$results %>% select(c("RMSE", "Rsquared", "MAE"))
M3_metrics <- M3_cv$results %>% select(c("RMSE", "Rsquared", "MAE"))
M4_metrics <- M4_cv$results %>% select(c("RMSE", "Rsquared", "MAE"))

cv_df <- merge(M1_metrics, M2_metrics, by=c("RMSE", "Rsquared", "MAE"), all = TRUE)
cv_df2 <- merge(cv_df, M3_metrics, by=c("RMSE", "Rsquared", "MAE"), all = TRUE)
total_cv <- merge(cv_df2, M4_metrics, by=c("RMSE", "Rsquared", "MAE"), all = TRUE)
total_cv <- total_cv %>% mutate(Model = c('M1', 'M2', 'M3', 'M4')) %>% relocate(Model, .before = RMSE)
total_cv %>% kable(digits = 3)

```

## Results
We made predictions on the hold-out set in data-test.csv, and generated a csv file containing the predictions for the first, second, third and fourth moments.

```{r, warning=FALSE}
data_test_transformed <- data_test %>%
  mutate(Re_category = case_when(Re == 90 ~ "Low", Re==224 ~ "Medium", Re == 398 ~ "High"))%>%
  mutate(Fr_transformed = invlogit(Fr))%>%
  mutate(Fr_category = case_when(Fr == 0.052 ~ "Low", Fr == 0.3 ~ "Medium", Fr == Inf ~ "High"))%>%
  mutate(Predicted_M1 = predict(final_model_M1, .))%>%
  mutate(Predicted_M2 = predict(final_model_M2, .))%>%
  mutate(Predicted_M3 = predict(final_model_M3, .))%>%
  mutate(Predicted_M4 = predict(final_model_M4, .))

data_predicted_output <- data_test_transformed%>%select(St, Re, Fr, Predicted_M1, Predicted_M2, Predicted_M3, Predicted_M4)
write.csv(data_predicted_output,"data-predict.csv", row.names = FALSE)
data_predicted_output %>% kable(digits = 5)
```

Major observations from the results:

- 

results section discussing your predictive results (donâ€™t forget uncertainty!), as
well as insights on the scientific problem.

## Conclusion




