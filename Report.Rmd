---
title: "Case Study Report"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, message=F, warning=F, echo=F}
require(rstan)
require(tidyverse)
require(rstanarm)
require(magrittr)
library(corrplot)
library(glmnet)
require(CRAN)
library(knitr)
library(caret)
library(dplyr)
library(jtools)
library(sjPlot)
library(sjmisc)
library(gridExtra)
```

```{r include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r load-data, message = FALSE, echo=F}
data_train<-read_csv("data/data-train.csv")
data_test<-read_csv("data/data-test.csv")
```

## Introduction

The objective of this case study is to develop a predictive model to predict the distributions of particle clusters in turbulence from three predictors: fluid turbulence characterized by Reynold's number $Re$, gravitational acceleration $Fr$, and particle characteristics (size or density which is quantified by Stokes number $St$). We also want to understand how does each of the three parameters affect the distribution of particle cluster volumns. 

Developing an understanding of turbulence is important because the effects of turbulence are present in a wide variety of problems. For example, the distribution of ash in the atmosphere is controlled by atmospheric turbulence, which has many implications for the environment and aviation. Turbulence also controls the population dynamics of plankton, which play an important role in the carbon cycle. On a more cosmological and atmospheric level, turbulence plays a central part in the dispersion of ash in the atmosphere which has many implications for the environment and aviation as well as the thermodynamics of clouds, radioactive properties, and the rate at which droplets grow to form rain. 

We'll build a supervised machine learning model to give prediction on the complex physical phenomenon, and interpret the variables' relationship. We first conducted several necessary transformation on the variables. After trying several methods, we decided to use a linear model with interaction terms, and we performed 5-folds cross validation on the linear model to assure that our model has good predicting ability. 

The model we have decided to use to predict turbulence is a linear regression with all three variables and interaction terms, shown below:

$St+ Re_{category} +Fr_{transformed} +Fr_{transformed} \times Re_{category}+St \times Re_{category}$

where $Re_{category}$ is a categorical variable that classifies the training $Re$ into three categories ($Re = 90$ : Low, $Re = 224$ : Medium, $Re = 398$ : High) and $Fr_{transformed}$ is the inverse logit of $Fr$ to handle infinity values present in the data. The model includes interactions between gravitational acceleration and fluid turbulence, and between fluid turbulence and particle characteristics.

## Methodology

### EDA

After loading the data, we performed exploratory data analysis on all three predictors and four moments.


```{r, fig.width=6, fig.height=3}
p1 <- ggplot(data = data_train, mapping = aes(x = Fr)) + 
  geom_histogram() + labs(title = "Distribution of Fr")

p2 <- ggplot(data = data_train, mapping = aes(x = St)) + 
  geom_histogram() + labs(title = "Distribution of St")

p3 <- ggplot(data = data_train, mapping = aes(x = Re)) + 
  geom_histogram() + labs(title = "Distribution of Re")

grid.arrange(p1, p2, p3, nrow = 1)
```
From observing the data set and the histogram plots, we can see that there are some data transformation needed. First, Fr has Inf value, which can't be quantified, so Fr only has two values in the histogram. We have two options: 1.performing logit transformation on Fr to transform Inf to 1; 2.transforming Fr to categorical variables.

We can do this because the physicist only need to predict Fr on these three levels, so we don't need to consider extrapolation. Second, Re only has three levels. We also decided to convert Re to categorical variables, because of the same reasons as Fr.

We also found that the moments are highly linearly correlated:

```{r, echo=F, fig.width=4, fig.height=4} 
pairs(data_train[4:7], cex = 0.5, pch = 19)
```

It is worth-noticing that since we are trying to understand the probability distribution of the particles in the flows, we may want to look into the central moments instead of the raw moments here, because central moments give us a more meaningful interpretation of the probability distribution. However, since the 1st central moment is always 0, we need to predict 1st raw moment and other three central moments separately. Since `C_moment_2`, `C_moment_3`, and `C_moment_4` are highly linearly correlated, we decided to fit a model on `C_moment_2`, which will give us the relationship of the predictor variables on the other moments due to the high linear correlation between the moments.

```{r, echo=FALSE}
data_train <- data_train %>%
  mutate(Re_category = case_when(Re == 90 ~ "Low", Re==224 ~ "Medium", Re == 398 ~ "High"))%>%
  mutate(Fr_transformed = invlogit(Fr))
```

```{r, echo=F}
data_train <- data_train %>% mutate(C_moment_2 = R_moment_2 - (R_moment_1)^2, 
                                    C_moment_3 = R_moment_3 - 3*(R_moment_1*R_moment_2) + 2*(R_moment_1)^3, 
                                    C_moment_4 = R_moment_4 - 4*R_moment_1*R_moment_3 + 6*(R_moment_1)^2*R_moment_2 - 3*(R_moment_1)^4)
```

### Linear Model (Inference)

```{r, echo=FALSE}
data_train <- data_train %>%
  mutate(Fr_category = case_when(Fr == 0.052 ~ "Low", Fr == 0.3 ~ "Medium", Fr == Inf ~ "High")) %>% mutate(Fr_low = ifelse(Fr_category == 'Low', 1, 0)) %>% mutate(Fr_medium = ifelse(Fr_category == 'Medium', 1, 0)) %>% mutate(Fr_high = ifelse(Fr_category == 'High', 1, 0)) %>% mutate(Re_low = ifelse(Re_category == 'Low', 1, 0)) %>% mutate(Re_medium = ifelse(Re_category == 'Medium', 1, 0)) %>% mutate(Re_high = ifelse(Re_category == 'High', 1, 0))

```


We started by building a preliminary linear model: 

$\widehat{2nd \ Central \ Moment} = \hat{St}+ \widehat{Re_{category}} +\widehat{Fr_{category}} + \epsilon$

We find that this basic model has very low Rsquared, so we dicided to increase the model complexity by adding interaction terms.

Since the 1st central moment is always 0, we need to predict 1st raw moment directly.
We fitted models on the same model to predict second, third and fourth moments due to the collinearity as explained above:

```{r}
final_model_M1 <- lm(R_moment_1 ~ St+ Re_category+Fr_transformed+Fr_transformed*Re_category+St*Re_category, data = data_train)
final_model_M2 <- lm(C_moment_2 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train)
final_model_M3 <- lm(C_moment_3 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train)
final_model_M4 <- lm(C_moment_4 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train)
```

Here we perform a 5-fold cross validation on the model. We chose 5 folds over 10 because the limited data available.

```{r, warning=FALSE, echo=F}
set.seed(123456)
train.control <- trainControl(method = "cv", number = 5)
M1_cv <- train(R_moment_1 ~ St+ Re_category+Fr_transformed+Fr_transformed*Re_category+St*Re_category, data = data_train, method = "lm", trControl = train.control)
M2_cv <- train(C_moment_2 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train, method = "lm", trControl = train.control)
M3_cv <- train(C_moment_3 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train, method = "lm", trControl = train.control)
M4_cv <- train(C_moment_4 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train, method = "lm", trControl = train.control)
```

```{r, echo=F}
library(knitr)
M1_metrics <- M1_cv$results[c("RMSE","Rsquared","MAE")]
M2_metrics <- M2_cv$results[c("RMSE", "Rsquared", "MAE")]
M3_metrics <- M3_cv$results[c("RMSE", "Rsquared", "MAE")]
M4_metrics <- M4_cv$results[c("RMSE", "Rsquared", "MAE")]
cv_df <- merge(M1_metrics, M2_metrics, by=c("RMSE", "Rsquared", "MAE"), all = TRUE)
cv_df2 <- merge(cv_df, M3_metrics, by=c("RMSE", "Rsquared", "MAE"), all = TRUE)
total_cv <- merge(cv_df2, M4_metrics, by=c("RMSE", "Rsquared", "MAE"), all = TRUE)
total_cv <- total_cv %>% mutate(Model = c('R_M1', 'C_M2', 'C_M3', 'C_M4')) %>% relocate(Model, .before = RMSE)
total_cv %>% kable(digits = 3)
```

### Ridge Model

We also explored shrinkage methods such as ridge regression. We chose ridge because we don't need to perform  However, since we know that the three predictors are all active so that we do not need predictor selection, so we attempted to fit a ridge regression model.

```{r, echo=FALSE}
x <- data.matrix(data_train[, c('Re_low', "Re_medium", 'Re_high', 'St', 'Fr_low', "Fr_medium", "Fr_high")])
```

```{r,echo=FALSE}
set.seed(123456)

train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
lambda_seq = 10^seq(10, -2, length = 100)
```

```{r, echo=FALSE}
y2 <- data_train$C_moment_2
ridge2 <- glmnet(x[train, ], y2[train], alpha = 0, lambda = lambda_seq)
y2.test = y2[test]
```

```{r, echo=FALSE}
set.seed(123456)
#perform k-fold cross-validation to find optimal lambda value
cv_ridge2 <- cv.glmnet(x[train,], y2[train], alpha = 0, folds = 5)
best_lambda2 <- cv_ridge2$lambda.min
```

```{r, echo=FALSE}
ridge.pred2 <- predict(ridge2, s = best_lambda2, newx = x[test,])
rmse_c2 <- sqrt(mean((ridge.pred2 - y2.test)^2))
```

```{r, echo=FALSE}
y3 <- data_train$C_moment_3

ridge3 <- glmnet(x[train,], y3[train], alpha = 0, lambda = lambda_seq)
```

```{r, echo=FALSE}
set.seed(123)
#perform k-fold cross-validation to find optimal lambda value
cv_ridge3 <- cv.glmnet(x[train,], y3[train], alpha = 0, folds = 5)
#find optimal lambda value that minimizes test MSE
best_lambda3 <- cv_ridge3$lambda.min
```

```{r, echo=FALSE}
y3.test <- y3[test]
ridge.pred3 <- predict(ridge3, s = best_lambda3, newx = x[test,])
rmse_c3 <- sqrt(mean((ridge.pred3 - y3.test)^2))
```


```{r, echo=FALSE}
y4 <- data_train$C_moment_4
```

```{r, echo=FALSE}
ridge4 <- glmnet(x[train,], y4[train], alpha = 0, lambda = lambda_seq)
```

```{r, echo=FALSE}
set.seed(123456)
#perform k-fold cross-validation to find optimal lambda value
cv_ridge4 <- cv.glmnet(x[train,], y4[train], alpha = 0, folds = 5)
#find optimal lambda value that minimizes test MSE
best_lambda4 <- cv_ridge4$lambda.min
```

```{r, echo=FALSE}
y4.test <- y4[test]
ridge.pred4 <- predict(ridge4, s = best_lambda4, newx = x[test,])
rmse_c4 <- sqrt(mean((ridge.pred4 - y4.test)^2))
```

```{r, echo=FALSE}
RidgeModel <- c("C_M2", "C_M3", "C_M4")
RMSE <- c(rmse_c2, rmse_c3, rmse_c4)

ridgedf <- data.frame(RidgeModel, RMSE) %>% kable()

ridgedf
```

We can see that the linear model performs better than the ridge model over all, so we 'll just use the linear model for prediction. Moreover, it is simpler to interpret the inference result with the linear model. So we can also use the linear model for interpreting the relationship between variables.

### Log-Transformed Model (Final Model)

We decided to log transform the second, third and fourth moment response variables in order to restrict the predictions of values to positive only, since the second, third and fourth moments cannnot be negative

```{r,echo=F}
data_train<-data_train%>%
  mutate(log_C_moment_2=log(C_moment_2))%>%
  mutate(log_C_moment_3=log(C_moment_3))%>%
  mutate(log_C_moment_4=log(C_moment_4))
```

```{r, echo=F}
final_model_M1 <- lm(R_moment_1 ~ St+ Re_category+Fr_transformed+Fr_transformed*Re_category+St*Re_category, data = data_train)
final_model_M2 <- lm(log_C_moment_2 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train)
final_model_M3 <- lm(log_C_moment_3 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train)
final_model_M4 <- lm(log_C_moment_4 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train)
summary(final_model_M1)$coefficients %>% kable(digits = 3)
summary(final_model_M2)$coefficients %>% kable(digits = 3)
summary(final_model_M3)$coefficients %>% kable(digits = 3)
summary(final_model_M4)$coefficients %>% kable(digits = 3)
```

However, the ridge

Then we fitted linear regression model on the original first moment response variable, and linear regression model on the log-transformed response variables for second, third and fourth moments.

## Results

We made predictions on the hold-out set in data-test.csv, and generated a csv file containing the predictions for the first, second, third and fourth moments.

```{r, warning=FALSE, echo=F}
data_test_transformed <- data_test %>%
  mutate(Re_category = case_when(Re == 90 ~ "Low", Re==224 ~ "Medium", Re == 398 ~ "High"))%>%
  mutate(Fr_transformed = invlogit(Fr))%>%
  mutate(Fr_category = case_when(Fr == 0.052 ~ "Low", Fr == 0.3 ~ "Medium", Fr == Inf ~ "High"))%>%
  mutate(Predicted_M1 = predict(final_model_M1, .))%>%
  mutate(Predicted_M2 = predict(final_model_M2, .))%>%
  mutate(Predicted_M3 = predict(final_model_M3, .))%>%
  mutate(Predicted_M4 = predict(final_model_M4, .))
                         
data_predicted_output <- data_test_transformed[c("St", "Re", "Re_category", "Fr","Fr_transformed","Fr_category", "Predicted_M1", "Predicted_M2", "Predicted_M3", "Predicted_M4")]%>%
  mutate(Predicted_R_M1 = exp(predict(final_model_M1, .)))%>%
  mutate(Predicted_C_M2 = exp(predict(final_model_M2, .)))%>%
  mutate(Predicted_C_M3 = exp(predict(final_model_M3, .)))%>%
  mutate(Predicted_C_M4 = exp(predict(final_model_M4, .)))%>%
  mutate(Predicted_R_M2 = Predicted_C_M2 + (Predicted_R_M1)^2) %>%
  mutate(Predicted_R_M3 = Predicted_C_M3 + 3*Predicted_R_M1*Predicted_R_M2 - 2*(Predicted_R_M1)^3) %>%
  mutate(Predicted_R_M4 = Predicted_C_M4 + 4*Predicted_R_M1*Predicted_R_M3 - 6*(Predicted_R_M1)^2*Predicted_R_M2 + 3*(Predicted_R_M1)^4)
data_predicted_output <- data_predicted_output[c("St", "Re", "Fr", "Predicted_R_M1", "Predicted_R_M2", "Predicted_R_M3", "Predicted_R_M4")]

write.csv(data_predicted_output,"data-predict.csv", row.names = FALSE)
data_predicted_output %>% kable(digits = 5)
```

Since the 1st central moment is always 0, we did not build a model for it. Instead, we directly predicts the 1st raw moment. There is a distinction between the three parameters' effects on mean and other three moments. When predicting the 1st raw moment, we did not transform Fr into categorical variables, and the interaction between Re and Fr has a significant negative, though weak, effects on the value of the mean.

The effects of three parameters are similar over other three central moments. So we'll take the 2nd central moment (variance) as a representative example. Some major observations from the results: First, Re is expected to have a negative relationship with the variance. The lower the Re, the larger the 2nd central moment. Second, St is expected to have a positive relationship with the 2nd central moment. Third, while Fr has a negative relationship with the variance, such negative effect is small when Fr number is high, and lower Fr number has stronger negative effects on the variance. It is worth-noticing that Fr does not have a significant main effect on the variance, given its high p-value. However, Fr's effects become significant in the interaction terms. 4.The interaction terms between Re and Fr has very strong positive effects on the 2nd central moment.

```{r, fig.width=3, fig.height=2}
plot_model(final_model_M2, type = "pred", terms = c("Fr_category", "Re_category"))
plot_model(final_model_M2, type = "pred", terms = c("St", "Re_category"))
```

Specifically, based on our modeling results, the two most significant terms are Re and the interaction between Re and Fr. Turbulence with Low Re is expected to have 3.82 unit higher variance than turbulence with High Re on average holding all else constant. The interaction between Low Re and Low Fr has strong positive effects on the variance. If the turbulence has low re and low fr, it is expected to have 13.06 unit higher variance than turbulence with Low Re and High Fr, holding all else constant. This result aligns with our prediction outcome--with 90 Re and 0.052 Fr, the distribution of particle cluster has incredibly high (419.49) variance.

We can now interpret the three parameters' effect in the physical context. Since Re (the Reynolds number) quantifies fluid turbulence, we can induce that the particle cluster volume distribution in turbulence has low uncertainty when Re is low. We can conclude that Laminar flows have low Re number, because the particle distribution is more orderly, regular, predictable. On the other hand, Turbulent flows have high Re number, because high Re is associated with high variance, thus the flows are more random and irregular. 

St (the Stokes number) is the ratio of the particle's momentum response time to the flow-field time scale. By definition, a larger Stokes number represents a larger or heavier particle. Our results demonstrate that particles with high St have greater impact on the turbulence. For small St, the particles will mostly follow the fluid motion, thus more predictable; for high St, the carrier fluid will have very limited influence on the particle motion, thus more unpredictable. We can conclude that Turbulent flows have high St, and Laminar flows have low St.

Fr (the Froud number) is the ratio of average flow velocity to the wave velocity in shallow water. So high Fr means fast rapid flow, and low Fr means slow tranquil flow. In our result, Fr in general has a negative effects on the variance, but such negative effects decreases while Fr increases. In other words, flows with high Fr is more unpredictable, and flows with low Fr is more orderly. Therefore, we can induce that Turbulent flow has high velocity, thus high Fr; Laminar flow has low velocity, thus low Fr.

The interaction between Re and Fr is significant in our results, so we can conclude that Re and Fr combining contribute to the dominant effects over the flow's motion, while the St is less significant.

## Conclusion

## Citations

https://www.sciencedirect.com/topics/engineering/stokes-number

https://www.sciencedirect.com/topics/engineering/froude-number

https://www.sciencedirect.com/topics/engineering/reynolds-number

