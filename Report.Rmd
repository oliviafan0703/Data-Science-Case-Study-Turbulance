---
title: "Case Study Report"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, message=F, warning=F, echo=F}
require(rstan)
require(tidyverse)
require(rstanarm)
require(magrittr)
library(corrplot)
library(caret)
library(glmnet)
require(CRAN)
library(knitr)
library(caret)
library(dplyr)
library(jtools)
library(sjPlot)
library(sjmisc)
library(gridExtra)
```

```{r include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r load-data, message = FALSE, echo=F}
data_train<-read_csv("data/data-train.csv")
data_test<-read_csv("data/data-test.csv")
```

```{r}
set.seed(2000)
```

## Introduction

The objective of this case study is to develop a predictive model to predict the distributions of particle clusters in turbulence from three predictors: fluid turbulence characterized by Reynold's number $Re$, gravitational acceleration $Fr$, and particle characteristics (size or density which is quantified by Stoke's number $St$). We also want to understand how does each of the three parameters affect the distribution of particle cluster volumns. 

Developing an understanding of turbulence is important because the effects of turbulence are present in a wide variety of problems. For example, the distribution of ash in the atmosphere is controlled by atmospheric turbulence as well as the thermodynamics of clouds, radioactive properties, and the rate at which droplets grow to form rain, which has many implications for the environment and aviation. Turbulence also controls the population dynamics of planktons, which play an important role in the carbon cycle. On a more cosmological level, turbulence also controls the dispersion of magnetism and heat from supernova events and possibly star formation.

We'll build a supervised machine learning model to give prediction on the complex physical phenomenon, and interpret the variables' relationship. We first conducted several necessary transformation on the variables. After trying several methods, we decided to use a linear model with interaction terms, and we performed 5-folds cross validation on the linear model to assure that our model has good predictive ability. 

## Methodology

### EDA

After loading the data, we performed exploratory data analysis on all three predictors and four moments.

```{r, fig.width=6, fig.height=3}
p1 <- ggplot(data = data_train, mapping = aes(x = Fr)) + 
  geom_histogram() + labs(title = "Distribution of Fr")

p2 <- ggplot(data = data_train, mapping = aes(x = St)) + 
  geom_histogram() + labs(title = "Distribution of St")

p3 <- ggplot(data = data_train, mapping = aes(x = Re)) + 
  geom_histogram() + labs(title = "Distribution of Re")

grid.arrange(p1, p2, p3, nrow = 1)
```

From observing the data set and the histogram plots, we can see that there are some data transformation needed. First, Fr has Inf value, which can't be quantified, so Fr only has two values in the histogram. We have two options: 1.performing logit transformation on Fr to transform Inf to 1; 2.transforming Fr to categorical variables.

We can do this because the physicist only need to predict Fr on these three levels, so we don't need to consider extrapolation. Second, Re only has three levels. We also decided to convert Re to categorical variables, because of the same reasons as Fr.

We also found that the moments are highly linearly correlated:

```{r, echo=F, fig.width=4, fig.height=4} 
pairs(data_train[4:7], cex = 0.5, pch = 19)
```

It is worth-noticing that since we are trying to understand the probability distribution of the particles in the flows, we may want to look into the central moments instead of the raw moments here, because central moments give us a more meaningful interpretation of the probability distribution. However, since the 1st central moment is always 0, we need to predict 1st raw moment and other three central moments separately. Since `C_moment_2`, `C_moment_3`, and `C_moment_4` are highly linearly correlated, we decided to fit a model on `C_moment_2`, which will give us the relationship of the predictor variables on the other moments due to the high linear correlation between the moments.

```{r, echo=FALSE}
data_train <- data_train %>%
  mutate(Re_category = case_when(Re == 90 ~ "Low", Re==224 ~ "Medium", Re == 398 ~ "High"))%>%
  mutate(Fr_transformed = invlogit(Fr))
```

```{r, echo=F}
data_train <- data_train %>% mutate(C_moment_2 = R_moment_2 - (R_moment_1)^2, 
                                    C_moment_3 = R_moment_3 - 3*(R_moment_1*R_moment_2) + 2*(R_moment_1)^3, 
                                    C_moment_4 = R_moment_4 - 4*R_moment_1*R_moment_3 + 6*(R_moment_1)^2*R_moment_2 - 3*(R_moment_1)^4)
```


### Preliminary Linear Model

```{r, echo=FALSE}
data_train <- data_train %>%
  mutate(Fr_category = case_when(Fr == 0.052 ~ "Low", Fr == 0.3 ~ "Medium", Fr == Inf ~ "High")) %>% mutate(Fr_low = ifelse(Fr_category == 'Low', 1, 0)) %>% mutate(Fr_medium = ifelse(Fr_category == 'Medium', 1, 0)) %>% mutate(Fr_high = ifelse(Fr_category == 'High', 1, 0)) %>% mutate(Re_low = ifelse(Re_category == 'Low', 1, 0)) %>% mutate(Re_medium = ifelse(Re_category == 'Medium', 1, 0)) %>% mutate(Re_high = ifelse(Re_category == 'High', 1, 0))

```

We started by building a preliminary linear model on the 2nd central moment:

$\widehat{2nd \ Central \ Moment} = \hat{St}+ \widehat{Re_{category}} +\widehat{Fr_{category}} + \epsilon$

```{r}
model <- lm(C_moment_2 ~ St + Fr_category + Re_category, data = data_train)
summ(model, vifs = TRUE)
```
From the result, we can see that multicollinearity is not a issue here. We also find that low Fr and low Re have significant effects on variance. Since this basic model has very low Rsquared (0.45), so we decided to increase the model complexity by adding interaction terms, especially exploring Fr and Re. 

Since the 1st central moment is always 0, we need to predict 1st raw moment directly.
We fitted models on the same model to predict second, third and fourth moments due to the collinearity as explained above:

```{r}
final_model_M1 <- lm(R_moment_1 ~ St+ Re_category+Fr_transformed+Fr_transformed*Re_category+St*Re_category, data = data_train)
final_model_M2 <- lm(C_moment_2 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train)
final_model_M3 <- lm(C_moment_3 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train)
final_model_M4 <- lm(C_moment_4 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train)
```

Here we perform a 5-fold cross validation on the model. We chose 5 folds over 10 because the limited data available.

```{r, warning=FALSE, echo=F}
set.seed(123456)
train.control <- trainControl(method = "cv", number = 5)
M1_cv <- train(R_moment_1 ~ St+ Re_category+Fr_transformed+Fr_transformed*Re_category+St*Re_category, data = data_train, method = "lm", trControl = train.control)
M2_cv <- train(C_moment_2 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train, method = "lm", trControl = train.control)
M3_cv <- train(C_moment_3 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train, method = "lm", trControl = train.control)
M4_cv <- train(C_moment_4 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train, method = "lm", trControl = train.control)
```

```{r, echo=F}
library(knitr)
M1_metrics <- M1_cv$results[c("RMSE","Rsquared","MAE")]
M2_metrics <- M2_cv$results[c("RMSE", "Rsquared", "MAE")]
M3_metrics <- M3_cv$results[c("RMSE", "Rsquared", "MAE")]
M4_metrics <- M4_cv$results[c("RMSE", "Rsquared", "MAE")]
cv_df <- merge(M1_metrics, M2_metrics, by=c("RMSE", "Rsquared", "MAE"), all = TRUE)
cv_df2 <- merge(cv_df, M3_metrics, by=c("RMSE", "Rsquared", "MAE"), all = TRUE)
total_cv <- merge(cv_df2, M4_metrics, by=c("RMSE", "Rsquared", "MAE"), all = TRUE)
total_cv <- total_cv %>% mutate(Model = c('R_M1', 'C_M2', 'C_M3', 'C_M4')) %>% relocate(Model, .before = RMSE)
total_cv %>% kable(digits = 3)
```

```{r, echo=FALSE}
x <- data.matrix(data_train[, c('Re_low', "Re_medium", 'Re_high', 'St', 'Fr_low', "Fr_medium", "Fr_high")])
```

```{r,echo=FALSE}
set.seed(123456)
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
lambda_seq = 10^seq(10, -2, length = 100)
```

```{r, echo=FALSE}
y2 <- data_train$C_moment_2
ridge2 <- glmnet(x[train, ], y2[train], alpha = 0, lambda = lambda_seq)
y2.test = y2[test]
```

```{r, echo=FALSE}
set.seed(123456)
#perform k-fold cross-validation to find optimal lambda value
cv_ridge2 <- cv.glmnet(x[train,], y2[train], alpha = 0, folds = 5)
best_lambda2 <- cv_ridge2$lambda.min
```

```{r, echo=FALSE}
ridge.pred2 <- predict(ridge2, s = best_lambda2, newx = x[test,])
rmse_c2 <- sqrt(mean((ridge.pred2 - y2.test)^2))
```

```{r, echo=FALSE}
y3 <- data_train$C_moment_3
ridge3 <- glmnet(x[train,], y3[train], alpha = 0, lambda = lambda_seq)
```

```{r, echo=FALSE}
set.seed(123)
#perform k-fold cross-validation to find optimal lambda value
cv_ridge3 <- cv.glmnet(x[train,], y3[train], alpha = 0, folds = 5)
#find optimal lambda value that minimizes test MSE
best_lambda3 <- cv_ridge3$lambda.min
```

```{r, echo=FALSE}
y3.test <- y3[test]
ridge.pred3 <- predict(ridge3, s = best_lambda3, newx = x[test,])
rmse_c3 <- sqrt(mean((ridge.pred3 - y3.test)^2))
```

```{r, echo=FALSE}
y4 <- data_train$C_moment_4
```

```{r, echo=FALSE}
ridge4 <- glmnet(x[train,], y4[train], alpha = 0, lambda = lambda_seq)
```

```{r, echo=FALSE}
set.seed(123456)
#perform k-fold cross-validation to find optimal lambda value
cv_ridge4 <- cv.glmnet(x[train,], y4[train], alpha = 0, folds = 5)
#find optimal lambda value that minimizes test MSE
best_lambda4 <- cv_ridge4$lambda.min
```

```{r, echo=FALSE}
y4.test <- y4[test]
ridge.pred4 <- predict(ridge4, s = best_lambda4, newx = x[test,])
rmse_c4 <- sqrt(mean((ridge.pred4 - y4.test)^2))
```

```{r, echo=FALSE}
RidgeModel <- c("C_M2", "C_M3", "C_M4")
RMSE <- c(rmse_c2, rmse_c3, rmse_c4)
ridgedf <- data.frame(RidgeModel, RMSE) %>% kable()
```


### Log-Transformed Model (Final Model)

We decided to log transform the second, third and fourth moment response variables in order to restrict the predictions of values to positive only, since the second, third and fourth moments cannnot be negative

```{r,echo=F}
data_train<-data_train%>%
  mutate(log_C_moment_2=log(C_moment_2))%>%
  mutate(log_C_moment_3=log(C_moment_3))%>%
  mutate(log_C_moment_4=log(C_moment_4))
```

Then we fitted linear regression model on the original first moment response variable, and linear regression model on the log-transformed response variables for second, third and fourth moments. All models have high rsquared (See appendix 1.1 for final model regression output).

```{r, echo=F}
final_model_M1 <- lm(R_moment_1 ~ St+ Re_category+Fr_transformed+Fr_transformed*Re_category+St*Re_category, data = data_train)
final_model_M2 <- lm(log_C_moment_2 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train)
final_model_M3 <- lm(log_C_moment_3 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train)
final_model_M4 <- lm(log_C_moment_4 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train)
m1_rsq <- summary(final_model_M1)$r.squared
m2_rsq <- summary(final_model_M2)$r.squared
m3_rsq <- summary(final_model_M3)$r.squared
m4_rsq <- summary(final_model_M4)$r.squared
Model <- c("R_M1", "C_M2", "C_M3", "C_M4")
Rsq <- c(m1_rsq, m2_rsq, m3_rsq, m4_rsq)
rsqdf <- data.frame(Model, Rsq) %>% kable()
rsqdf
```

The final equations are:

$R\_moment\_1=0.122 \times Re\_categoryLow +0.001 \times Re\_categoryMedium -0.052 \times Re\_categoryLow * Fr\_transformed +0.001 \times Re\_categoryMedium * Fr\_transformed + 0.028 \times St * Re\_categoryLow + 0.001 \times St * Re\_categoryMedium$

$C\_moment\_2 =\exp(-5.423+0.295 \times St +3.824\times Re\_categoryLow+ 1.130\times  Re\_categoryMedium-0.229 \times Fr\_categoryLow -0.067 \times Fr\_categoryMedium +6.886 \times Re\_categoryLow*Fr\_categoryLow+2.175 \times Re\_categoryMedium*Fr\_categoryLow+0.269 \times Re\_categoryLow*Fr\_categoryMedium+0.684 \times St*Re\_categoryLow+0.651 \times \hat{St}*Re\_categoryMedium)$

$C\_moment\_3=\exp(-2.474+0.315 \times St +2.874\times Re\_categoryLow+ 0.573\times  Re\_categoryMedium-0.289 \times Fr\_categoryLow-0.077 \times Fr\_categoryMedium +13.060 \times Re\_categoryLow*Fr\_categoryLow+4.303 \times Re\_categoryMedium*Fr\_categoryLow+0.307 \times Re\_categoryLow*Fr\_categoryMedium+1.116 \times St*Re\_categoryLow+1.002 \times St*Re\_categoryMedium)$

$C\_moment\_4=\exp(0.475+0.335 \times St +2.102\times Re\_categoryLow+ 0.096\times  Re\_categoryMedium-0.349 \times Fr\_categoryLow-0.097 \times Fr\_categoryMedium +19.153 \times Re\_categoryLow*Fr\_categoryLow+6.421 \times Re\_categoryMedium*Fr\_categoryLow+0.348 \times Re\_categoryLow*Fr\_categoryMedium+1.488 \times St*Re\_categoryLow+1.304 \times St*Re\_categoryMedium)$


## Results

We made predictions on the hold-out set in data-test.csv, and generated a csv file containing the predictions for the first, second, third and fourth moments.

```{r, warning=FALSE, echo=F}
data_test_transformed <- data_test %>%
  mutate(Re_category = case_when(Re == 90 ~ "Low", Re==224 ~ "Medium", Re == 398 ~ "High"))%>%
  mutate(Fr_transformed = invlogit(Fr))%>%
  mutate(Fr_category = case_when(Fr == 0.052 ~ "Low", Fr == 0.3 ~ "Medium", Fr == Inf ~ "High"))%>%
  mutate(Predicted_M1 = predict(final_model_M1, .))%>%
  mutate(Predicted_M2 = predict(final_model_M2, .))%>%
  mutate(Predicted_M3 = predict(final_model_M3, .))%>%
  mutate(Predicted_M4 = predict(final_model_M4, .))
                         
data_predicted_output <- data_test_transformed[c("St", "Re", "Re_category", "Fr","Fr_transformed","Fr_category", "Predicted_M1", "Predicted_M2", "Predicted_M3", "Predicted_M4")]%>%
  mutate(Predicted_R_M1 = exp(predict(final_model_M1, .)))%>%
  mutate(Predicted_C_M2 = exp(predict(final_model_M2, .)))%>%
  mutate(Predicted_C_M3 = exp(predict(final_model_M3, .)))%>%
  mutate(Predicted_C_M4 = exp(predict(final_model_M4, .)))%>%
  mutate(Predicted_R_M2 = Predicted_C_M2 + (Predicted_R_M1)^2) %>%
  mutate(Predicted_R_M3 = Predicted_C_M3 + 3*Predicted_R_M1*Predicted_R_M2 - 2*(Predicted_R_M1)^3) %>%
  mutate(Predicted_R_M4 = Predicted_C_M4 + 4*Predicted_R_M1*Predicted_R_M3 - 6*(Predicted_R_M1)^2*Predicted_R_M2 + 3*(Predicted_R_M1)^4)
data_predicted_output <- data_predicted_output[c("St", "Re", "Fr", "Predicted_R_M1", "Predicted_R_M2", "Predicted_R_M3", "Predicted_R_M4")]

write.csv(data_predicted_output,"data-predict.csv", row.names = FALSE)
```

Since the 1st central moment is always 0, we did not build a model for it. Instead, we directly predicts the 1st raw moment. There is a distinction between the three parameters' effects on mean and other three moments. When predicting the 1st raw moment, we did not transform Fr into categorical variables, and the interaction between Re and Fr has a significant negative, though weak, effects on the value of the mean.

The effects of three parameters are similar over other three central moments. Some major observations from the results: First, Re is expected to have a negative relationship with the variance, skewness, and kurtosis. The lower the Re, the greater the 2nd, 3rd, and 4th central moments. Second, St is expected to have a positive relationship with the 2nd, 3rd, and 4th central moments. Third, while Fr has a negative relationship with the moments, such negative effect decreases while Fr increases, so lower Fr is associated with high variance, skewness, and kurtosis. It is worth-noticing that Fr independently does not have a significant main effect on the moments, given its high p-value. However, Fr's effects become significant in the interaction terms. 4.The interaction terms between Re and Fr has very strong positive effects on the moments.

```{r, fig.width=3, fig.height=2}
plot_model(final_model_M2, type = "pred", terms = c("Fr_category", "Re_category"))
plot_model(final_model_M2, type = "pred", terms = c("St", "Re_category"))
```

Specifically, based on our modeling results, the two most significant terms are Re and the interaction between Re and Fr. Taking the 2nd central moment as an example, turbulence with low Re is expected to have 3.82 unit higher variance in particle distribution than that of turbulence with high Re on average holding all else constant. The interaction between low Re and low Fr has strong positive effects on the variance of particle distribution. If the turbulence has low re and low fr, its particle distribution is expected to have 13.06 unit higher variance than turbulence with low Re and high Fr, holding all else constant. This result aligns with our prediction outcome--with 90 Re and 0.052 Fr, the distribution of particle cluster has incredibly high variance (419.49), high skewness(2.194687e+06), and high kurtosis(1.195146e+10). 

We can now interpret the three parameters' effect in the physical context. Since Re (the Reynolds number) quantifies fluid turbulence, the higher the Re, the more turbulent the flow. We can conclude that the particle cluster volume distribution has lower variance, skewness, and kurtosis when Re is high, which means that the turbulent flows' particles distribute in a more normal, symmetric way with few outliers.

St (the Stokes number) is the ratio of the particle's momentum response time to the flow-field time scale. By definition, a larger Stokes number represents a larger or heavier particle. Turbulent flows have high St, and Laminar flows have low St. Our results demonstrate that particle distributions with lower St numbers have smaller variance, skewness, and kurtosis. So with small St, the particles will mostly distribute normally and symmetrically and with few outliers. 

Fr (the Froud number) is the ratio of average flow velocity to the wave velocity in shallow water. So high Fr means fast rapid flow (turbulent), and low Fr means slow tranquil flow (laminar). In our result, Fr in general has a negative effects on the variance, but such negative effects decreases while Fr increases. In other words, higher Fr is associated with higher variance. Therefore, with low Fr, the particles distribute in a more normal, symmetric way with few outliers; with high Fr, the particles distribution has greater variance.

The interaction between Re and Fr is significant in our results, so we can conclude that Re and Fr combining is very important in affecting particles' distribution motion, while the effects of St are less significant.

From the SE table (Appendix 1.4), we can see that the 1st raw moment has very small standard error and low uncertainty. The uncertainty increases as moments increases--higher order moments generally have higher uncertainty than lower order moments. It is worth-noticing that SE is the highest when St, Fr, Re are all high. This indicates that our prediction is more reliable when the levels of three parameters vary. 

## Conclusion

Since our response variables, where highly linearly correlated, we came to the conclusion that the model which best fit the first moment would also be a good fit for the other models. We saw that after transforming Fr and Re into categorical variables, we greatly increased the Test MSE of our models. Taking the second central moment as an example, we saw some statistically significant interaction terms between Re and Fr, which, because of the hierarchy principle, means we included the order 1 terms Re_category and Fr_categrory as well. We predicted that the variance of the particle cluster would increase significantly if the gravitational acceleration and Reynold's number jointly decreased. When fitting a model only with the linear terms St, Re_category, and Fr_category, we saw that there was a statistically significant increase in the variance when the Reynold's number and gravitational acceleration decreased independently, but to a much lesser extent than with the interaction term.

In this study, we analyzed the effect of Re, Fr, and St to the distribution of particle cluster volumns. It is important to note some limitation of our results: First, we have a relatively small training dataset (<100). This might result in issues relating to generalization and data imbalance. Second, we only have three levels for Re and Fr. There will be a serious issue of bad extrapolation if we use this model to predict tuples with Re and Fr outside of these three levels. Therefore, this model is only reliable with these three levels of Re and Fr. Last but not least, although our models has high Rsquared for all 4 moments, our model is a linear model with couple interaction terms, but based on our EDA, the relationship between Fr/Re and the moments is not linear. So we think there should be more complex models that can better describe the relationship between variables, thus produce a more accurate prediction result.

We believe that our model provides good prediction and on the probability distribution of particle cluster volumns, and interpretation on how Re, Fr, St affect the flows. We conclude that Turbulent flows have high St, high Re, high Fr; Laminar flows have low St, low Re, low Fr. Our results also reveal that the interaction between Re and Fr has significant effects on flows' motion. 

Moreover, we think that there might be other omitted variables that affect the flow motion. We are curious about the threshold where flows transitioning from laminar to turbulent. In future study, we hope to explore other potential predictor variables and the critical point that decides flow's type.

## Citations

https://www.sciencedirect.com/topics/engineering/stokes-number

https://www.sciencedirect.com/topics/engineering/froude-number

https://www.sciencedirect.com/topics/engineering/reynolds-number

\newpage

## Appendix

### 1.1

The regression output for models on 1st raw moment and 2nd, 3rd, 4th central moment.
```{r}
summary(final_model_M1)$coefficients %>% kable(digits = 3)
summary(final_model_M2)$coefficients %>% kable(digits = 3)
summary(final_model_M3)$coefficients %>% kable(digits = 3)
summary(final_model_M4)$coefficients %>% kable(digits = 3)
```

### 1.2

2. The prediction output on the test data. 
```{r}
data_predicted_output %>% kable(digits = 5)
```

### 1.3
### Model Diagnostics

```{r, echo=F} 
par(mfrow = c(2, 4))
plot(final_model_M1)
plot(final_model_M2)
plot(final_model_M3)
plot(final_model_M4)
```

We conducted model diagnostic on the final model from mainly four perspectives

- Residuals vs Fitted: While 
Used to check the linear relationship assumptions. A horizontal line, without distinct patterns is an indication for a linear relationship, what is good.

- Normal Q-Q. Used to examine whether the residuals are normally distributed. It’s good if residuals points follow the straight dashed line.

- Scale-Location (or Spread-Location). Used to check the homogeneity of variance of the residuals (homoscedasticity). Horizontal line with equally spread points is a good indication of homoscedasticity. This is not the case in our example, where we have a heteroscedasticity problem.

- Residuals vs Leverage. Used to identify influential cases, that is extreme values that might influence the regression results when included or excluded from the analysis. This plot will be described further in the next sections.

### 1.4
### SE Table
```{r}
SE_M1 <- predict(final_model_M1, data_test_transformed, se.fit = TRUE)$se.fit
SE_M2 <- predict(final_model_M2, data_test_transformed, se.fit = TRUE)$se.fit
SE_M3 <- predict(final_model_M3, data_test_transformed, se.fit = TRUE)$se.fit
SE_M4 <- predict(final_model_M4, data_test_transformed, se.fit = TRUE)$se.fit

se.table = data.frame(SE_M1, SE_M2, SE_M3, SE_M4)

se.table
```

