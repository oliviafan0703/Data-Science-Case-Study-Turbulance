---
title: "Case Study Report"
author: "Alicia Gong, Olivia Fan, Jake Heller, Annie Do"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, message=F, warning=F, echo=F}
require(rstan)
require(tidyverse)
require(rstanarm)
require(magrittr)
library(corrplot)
library(caret)
library(glmnet)
require(CRAN)
library(knitr)
library(caret)
library(dplyr)
library(jtools)
library(sjPlot)
library(sjmisc)
library(gridExtra)
```

```{r include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r load-data, message = FALSE, echo=F}
data_train<-read_csv("data/data-train.csv")
data_test<-read_csv("data/data-test.csv")
```

```{r}
set.seed(2000)
```

## Introduction

The objective of this case study is to develop a predictive model to predict the distributions of particle clusters in turbulence from three predictors: fluid turbulence characterized by Reynold's number $Re$, gravitational acceleration $Fr$, and particle characteristics (size or density which is quantified by Stoke's number $St$). We also want to understand how each of the three parameters affects the distribution of particle cluster volumns. 

Developing an understanding of turbulence is important because the effects of turbulence are present in a wide variety of problems. For example, the distribution of ash in the atmosphere is controlled by atmospheric turbulence as well as the thermodynamics of clouds, radioactive properties, and the rate at which droplets grow to form rain, which has many implications for the environment and aviation. Turbulence also controls the population dynamics of planktons, which play an important role in the carbon cycle. On a more cosmological level, turbulence also controls the dispersion of magnetism and heat from supernova events and possibly star formation.

We'll build a supervised machine learning model to give prediction on the complex physical phenomenon, and interpret the variables' relationship. We first conducted several necessary transformation on the variables. After trying several methods, we decided to use a linear model with interaction terms, and we performed 5-folds cross validation on the linear model to assure that our model has good predictive ability. 

## Methodology

### EDA

After loading the data, we performed exploratory data analysis on all three predictors and four moments.

```{r, fig.width=6, fig.height=2}
p1 <- ggplot(data = data_train, mapping = aes(x = Fr)) + 
  geom_histogram() + labs(title = "Distribution of Fr")

p2 <- ggplot(data = data_train, mapping = aes(x = St)) + 
  geom_histogram() + labs(title = "Distribution of St")

p3 <- ggplot(data = data_train, mapping = aes(x = Re)) + 
  geom_histogram() + labs(title = "Distribution of Re")

grid.arrange(p1, p2, p3, nrow = 1)
```

From observing the data set and the histogram plots, we can see that there are some data transformation needed. First, Fr has Inf value, which can't be quantified, so Fr only has two values in the histogram. We have two options: 1) performing logit transformation on Fr to transform Inf to 1; 2) transforming Fr to categorical variables.

We can do this because the physicist only need to predict Fr on these three levels, so we don't need to consider extrapolation. Second, Re only has three levels. We also decided to convert Re to categorical variables, because of the same reasons as Fr.

We also found that the moments are highly linearly correlated:

```{r, echo=F, fig.width=4, fig.height=4} 
pairs(data_train[4:7], cex = 0.5, pch = 19)
```

It is worth-noticing that since we are trying to understand the probability distribution of the particles in the flows, we may want to look into the central moments instead of the raw moments here, because central moments give us a more meaningful interpretation of the probability distribution. However, since the 1st central moment is always 0, we need to predict 1st raw moment and other three central moments separately. Since `C_moment_2`, `C_moment_3`, and `C_moment_4` are highly linearly correlated, we decided to fit a model on `C_moment_2`, which will give us the relationship of the predictor variables on the other moments due to the high linear correlation between the moments.

```{r, echo=FALSE}
data_train <- data_train %>%
  mutate(Re_category = case_when(Re == 90 ~ "Low", Re==224 ~ "Medium", Re == 398 ~ "High"))%>%
  mutate(Fr_transformed = invlogit(Fr))
```

```{r, echo=F}
data_train <- data_train %>% mutate(C_moment_2 = R_moment_2 - (R_moment_1)^2, 
                                    C_moment_3 = R_moment_3 - 3*(R_moment_1*R_moment_2) + 2*(R_moment_1)^3, 
                                    C_moment_4 = R_moment_4 - 4*R_moment_1*R_moment_3 + 6*(R_moment_1)^2*R_moment_2 - 3*(R_moment_1)^4)
```


### Preliminary Linear Model

```{r, echo=FALSE}
data_train <- data_train %>%
  mutate(Fr_category = case_when(Fr == 0.052 ~ "Low", Fr == 0.3 ~ "Medium", Fr == Inf ~ "High")) %>% mutate(Fr_low = ifelse(Fr_category == 'Low', 1, 0)) %>% mutate(Fr_medium = ifelse(Fr_category == 'Medium', 1, 0)) %>% mutate(Fr_high = ifelse(Fr_category == 'High', 1, 0)) %>% mutate(Re_low = ifelse(Re_category == 'Low', 1, 0)) %>% mutate(Re_medium = ifelse(Re_category == 'Medium', 1, 0)) %>% mutate(Re_high = ifelse(Re_category == 'High', 1, 0))

```

We started by building a preliminary linear model on the 2nd central moment:

$\widehat{2nd \ Central \ Moment} = \hat{St}+ \widehat{Re_{category}} +\widehat{Fr_{category}} + \epsilon$

```{r}
model <- lm(C_moment_2 ~ St + Fr_category + Re_category, data = data_train)
summ(model, vifs = TRUE)
```
From the result, we can see that multicollinearity is not a issue here. We also find that low Fr and low Re have significant effects on variance. Since this basic model has very low Rsquared (0.45), so we decided to increase the model complexity by adding interaction terms, especially exploring Fr and Re. 

Since the 1st central moment is always 0, we need to predict 1st raw moment directly.
We fitted models on the same model to predict second, third and fourth moments due to the collinearity as explained above:

```{r}
final_model_M1 <- lm(R_moment_1 ~ St+ Re_category+Fr_transformed+Fr_transformed*Re_category+St*Re_category, data = data_train)
final_model_M2 <- lm(C_moment_2 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train)
final_model_M3 <- lm(C_moment_3 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train)
final_model_M4 <- lm(C_moment_4 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train)
```

Here we perform a 5-fold cross validation on the model. We chose 5 folds over 10 because the limited data available.

```{r, warning=FALSE, echo=F}
set.seed(123456)
train.control <- trainControl(method = "cv", number = 5)
M1_cv <- train(R_moment_1 ~ St+ Re_category+Fr_transformed+Fr_transformed*Re_category+St*Re_category, data = data_train, method = "lm", trControl = train.control)
M2_cv <- train(C_moment_2 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train, method = "lm", trControl = train.control)
M3_cv <- train(C_moment_3 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train, method = "lm", trControl = train.control)
M4_cv <- train(C_moment_4 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train, method = "lm", trControl = train.control)
```

```{r, echo=F}
library(knitr)
M1_metrics <- M1_cv$results[c("RMSE","Rsquared","MAE")]
M2_metrics <- M2_cv$results[c("RMSE", "Rsquared", "MAE")]
M3_metrics <- M3_cv$results[c("RMSE", "Rsquared", "MAE")]
M4_metrics <- M4_cv$results[c("RMSE", "Rsquared", "MAE")]
cv_df <- merge(M1_metrics, M2_metrics, by=c("RMSE", "Rsquared", "MAE"), all = TRUE)
cv_df2 <- merge(cv_df, M3_metrics, by=c("RMSE", "Rsquared", "MAE"), all = TRUE)
total_cv <- merge(cv_df2, M4_metrics, by=c("RMSE", "Rsquared", "MAE"), all = TRUE)
total_cv <- total_cv %>% mutate(Model = c('R_M1', 'C_M2', 'C_M3', 'C_M4')) %>% relocate(Model, .before = RMSE)
total_cv %>% kable(digits = 3)
```

```{r, echo=FALSE}
x <- data.matrix(data_train[, c('Re_low', "Re_medium", 'Re_high', 'St', 'Fr_low', "Fr_medium", "Fr_high")])
```

```{r,echo=FALSE}
set.seed(123456)
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
lambda_seq = 10^seq(10, -2, length = 100)
```

```{r, echo=FALSE}
y2 <- data_train$C_moment_2
ridge2 <- glmnet(x[train, ], y2[train], alpha = 0, lambda = lambda_seq)
y2.test = y2[test]
```

```{r, echo=FALSE}
set.seed(123456)
#perform k-fold cross-validation to find optimal lambda value
cv_ridge2 <- cv.glmnet(x[train,], y2[train], alpha = 0, folds = 5)
best_lambda2 <- cv_ridge2$lambda.min
```

```{r, echo=FALSE}
ridge.pred2 <- predict(ridge2, s = best_lambda2, newx = x[test,])
rmse_c2 <- sqrt(mean((ridge.pred2 - y2.test)^2))
```

```{r, echo=FALSE}
y3 <- data_train$C_moment_3
ridge3 <- glmnet(x[train,], y3[train], alpha = 0, lambda = lambda_seq)
```

```{r, echo=FALSE}
set.seed(123)
#perform k-fold cross-validation to find optimal lambda value
cv_ridge3 <- cv.glmnet(x[train,], y3[train], alpha = 0, folds = 5)
#find optimal lambda value that minimizes test MSE
best_lambda3 <- cv_ridge3$lambda.min
```

```{r, echo=FALSE}
y3.test <- y3[test]
ridge.pred3 <- predict(ridge3, s = best_lambda3, newx = x[test,])
rmse_c3 <- sqrt(mean((ridge.pred3 - y3.test)^2))
```

```{r, echo=FALSE}
y4 <- data_train$C_moment_4
```

```{r, echo=FALSE}
ridge4 <- glmnet(x[train,], y4[train], alpha = 0, lambda = lambda_seq)
```

```{r, echo=FALSE}
set.seed(123456)
#perform k-fold cross-validation to find optimal lambda value
cv_ridge4 <- cv.glmnet(x[train,], y4[train], alpha = 0, folds = 5)
#find optimal lambda value that minimizes test MSE
best_lambda4 <- cv_ridge4$lambda.min
```

```{r, echo=FALSE}
y4.test <- y4[test]
ridge.pred4 <- predict(ridge4, s = best_lambda4, newx = x[test,])
rmse_c4 <- sqrt(mean((ridge.pred4 - y4.test)^2))
```

```{r, echo=FALSE}
RidgeModel <- c("C_M2", "C_M3", "C_M4")
RMSE <- c(rmse_c2, rmse_c3, rmse_c4)
ridgedf <- data.frame(RidgeModel, RMSE) %>% kable()
```


### Log-Transformed Model (Final Model)

We decided to log transform the second, third and fourth moment response variables in order to restrict the predictions of values to positive only, since the second, third and fourth moments cannnot be negative.

```{r,echo=F}
data_train<-data_train%>%
  mutate(log_C_moment_2=log(C_moment_2))%>%
  mutate(log_C_moment_3=log(C_moment_3))%>%
  mutate(log_C_moment_4=log(C_moment_4))
```

Then we fitted linear regression model on the original first moment response variable, and linear regression model on the log-transformed response variables for second, third and fourth moments. All models have high $R^2$ values (See appendix 1.1 for final model regression output).

```{r, echo=F}
final_model_M1 <- lm(R_moment_1 ~ St+ Re_category+Fr_transformed+Fr_transformed*Re_category+St*Re_category, data = data_train)
final_model_M2 <- lm(log_C_moment_2 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train)
final_model_M3 <- lm(log_C_moment_3 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train)
final_model_M4 <- lm(log_C_moment_4 ~ St+ Re_category+Fr_category+Fr_category*Re_category+St*Re_category, data = data_train)
m1_rsq <- summary(final_model_M1)$r.squared
m2_rsq <- summary(final_model_M2)$r.squared
m3_rsq <- summary(final_model_M3)$r.squared
m4_rsq <- summary(final_model_M4)$r.squared
Model <- c("R_M1", "C_M2", "C_M3", "C_M4")
Rsq <- c(m1_rsq, m2_rsq, m3_rsq, m4_rsq)
rsqdf <- data.frame(Model, Rsq) %>% kable()
rsqdf
```

The final equations are:

$R\_moment\_1=0.122 \times Re\_categoryLow +0.001 \times Re\_categoryMedium -0.052 \times Re\_categoryLow * Fr\_transformed +0.001 \times Re\_categoryMedium * Fr\_transformed + 0.028 \times St * Re\_categoryLow + 0.001 \times St * Re\_categoryMedium$

$C\_moment\_2 =\exp(-5.423+0.295 \times St +3.824\times Re\_categoryLow+ 1.130\times  Re\_categoryMedium-0.229 \times Fr\_categoryLow -0.067 \times Fr\_categoryMedium +6.886 \times Re\_categoryLow*Fr\_categoryLow+2.175 \times Re\_categoryMedium*Fr\_categoryLow+0.269 \times Re\_categoryLow*Fr\_categoryMedium+0.684 \times St*Re\_categoryLow+0.651 \times \hat{St}*Re\_categoryMedium)$

$C\_moment\_3=\exp(-2.474+0.315 \times St +2.874\times Re\_categoryLow+ 0.573\times  Re\_categoryMedium-0.289 \times Fr\_categoryLow-0.077 \times Fr\_categoryMedium +13.060 \times Re\_categoryLow*Fr\_categoryLow+4.303 \times Re\_categoryMedium*Fr\_categoryLow+0.307 \times Re\_categoryLow*Fr\_categoryMedium+1.116 \times St*Re\_categoryLow+1.002 \times St*Re\_categoryMedium)$

$C\_moment\_4=\exp(0.475+0.335 \times St +2.102\times Re\_categoryLow+ 0.096\times  Re\_categoryMedium-0.349 \times Fr\_categoryLow-0.097 \times Fr\_categoryMedium +19.153 \times Re\_categoryLow*Fr\_categoryLow+6.421 \times Re\_categoryMedium*Fr\_categoryLow+0.348 \times Re\_categoryLow*Fr\_categoryMedium+1.488 \times St*Re\_categoryLow+1.304 \times St*Re\_categoryMedium)$

We performed model diagnostics on the generalized linear model, as discussed in Appendix 1.4.

\newpage
## Results

We made predictions on the hold-out set in data-test.csv, and generated a csv file containing the predictions for the first, second, third and fourth moments.

```{r, warning=FALSE, echo=F}
data_test_transformed <- data_test %>%
  mutate(Re_category = case_when(Re == 90 ~ "Low", Re==224 ~ "Medium", Re == 398 ~ "High"))%>%
  mutate(Fr_transformed = invlogit(Fr))%>%
  mutate(Fr_category = case_when(Fr == 0.052 ~ "Low", Fr == 0.3 ~ "Medium", Fr == Inf ~ "High"))%>%
  mutate(Predicted_M1 = predict(final_model_M1, .))%>%
  mutate(Predicted_M2 = predict(final_model_M2, .))%>%
  mutate(Predicted_M3 = predict(final_model_M3, .))%>%
  mutate(Predicted_M4 = predict(final_model_M4, .))
                         
data_predicted_output <- data_test_transformed[c("St", "Re", "Re_category", "Fr","Fr_transformed","Fr_category", "Predicted_M1", "Predicted_M2", "Predicted_M3", "Predicted_M4")]%>%
  mutate(Predicted_R_M1 = exp(predict(final_model_M1, .)))%>%
  mutate(Predicted_C_M2 = exp(predict(final_model_M2, .)))%>%
  mutate(Predicted_C_M3 = exp(predict(final_model_M3, .)))%>%
  mutate(Predicted_C_M4 = exp(predict(final_model_M4, .)))%>%
  mutate(Predicted_R_M2 = Predicted_C_M2 + (Predicted_R_M1)^2) %>%
  mutate(Predicted_R_M3 = Predicted_C_M3 + 3*Predicted_R_M1*Predicted_R_M2 - 2*(Predicted_R_M1)^3) %>%
  mutate(Predicted_R_M4 = Predicted_C_M4 + 4*Predicted_R_M1*Predicted_R_M3 - 6*(Predicted_R_M1)^2*Predicted_R_M2 + 3*(Predicted_R_M1)^4)
data_predicted_output <- data_predicted_output[c("St", "Re", "Fr", "Predicted_R_M1", "Predicted_R_M2", "Predicted_R_M3", "Predicted_R_M4")]

write.csv(data_predicted_output,"data-predict.csv", row.names = FALSE)
```

Since the 1st central moment is always 0, we did not build a model for it. Instead, we directly predicts the 1st raw moment. There is a distinction between the three parameters' effects on mean and other three moments. When predicting the 1st raw moment, we did not transform Fr into categorical variables, and the interaction between Re and Fr has a significant negative, though weak, effects on the value of the mean.

The effects of three parameters are similar over other three central moments. Some major observations from the results: First, Re is expected to have a negative relationship with the variance, skewness, and kurtosis. The lower the Re, the greater the 2nd, 3rd, and 4th central moments. Second, St is expected to have a positive relationship with the 2nd, 3rd, and 4th central moments. Third, while Fr has a negative relationship with the moments, such negative effect decreases while Fr increases, so lower Fr is associated with high variance, skewness, and kurtosis. It is worth-noticing that Fr independently does not have a significant main effect on the moments, given its high p-value. However, Fr's effects become significant in the interaction terms. 4.The interaction terms between Re and Fr has very strong positive effects on the moments.

```{r, fig.width=3, fig.height=2}
plot_model(final_model_M2, type = "pred", terms = c("Fr_category", "Re_category"))
plot_model(final_model_M2, type = "pred", terms = c("St", "Re_category"))
```

Specifically, based on our modeling results, the two most significant terms are Re and the interaction between Re and Fr. Taking the 2nd central moment as an example, turbulence with low Re is expected to have 3.82 unit higher variance in particle distribution than that of turbulence with high Re on average holding all else constant. The interaction between low Re and low Fr has strong positive effects on the variance of particle distribution. If the turbulence has low re and low fr, its particle distribution is expected to have 13.06 unit higher variance than turbulence with low Re and high Fr, holding all else constant. This result aligns with our prediction outcome--with 90 Re and 0.052 Fr, the distribution of particle cluster has incredibly high variance (419.49), high skewness(2.194687e+06), and high kurtosis(1.195146e+10). 

We can now interpret the three parameters' effect in the physical context. Since Re (the Reynolds number) quantifies fluid turbulence, the higher the Re, the more turbulent the flow. We can conclude that the particle cluster volume distribution has lower variance, skewness, and kurtosis when Re is high, which means that the turbulent flows' particles distribute in a more normal, symmetric way with few outliers.

St (the Stokes number) is the ratio of the particle's momentum response time to the flow-field time scale. By definition, a larger Stokes number represents a larger or heavier particle. Turbulent flows have high St, and Laminar flows have low St. Our results demonstrate that particle distributions with lower St numbers have smaller variance, skewness, and kurtosis. So with small St, the particles will mostly distribute normally and symmetrically and with few outliers. 

Fr (the Froud number) is the ratio of average flow velocity to the wave velocity in shallow water. So high Fr means fast rapid flow (turbulent), and low Fr means slow tranquil flow (laminar). In our result, Fr in general has a negative effects on the variance, but such negative effects decreases while Fr increases. In other words, higher Fr is associated with higher variance. Therefore, with low Fr, the particles distribute in a more normal, symmetric way with few outliers; with high Fr, the particles distribution has greater variance.

The interaction between Re and Fr is significant in our results, so we can conclude that Re and Fr combining is very important in affecting particles' distribution motion, while the effects of St are less significant. When the flow is laminar, Re and Fr are both low, the probability distribution for particle cluster volumes is spread out, highly skewed, and is heavily influenced by outliers. When the flow is turbulent, the probability distribution for particle cluster volumes is fairly normal, symmetric, and with few outliers. We suspect that in the laminar flows, the particle disturbances are more localized, so this results in higher variances, skewness, and kurtosis in the particle distributions. On the other hand, in the turbulent flows, the particles disturbances are more randomized, so the particle distribution is more normal. 

From the SE table (Appendix 1.3), we can see that the 1st raw moment has very small standard error and low uncertainty. The uncertainty increases as moments increases--higher order moments generally have higher uncertainty than lower order moments. It is worth-noticing that SE is the highest when St, Fr, Re are all high. This indicates that our prediction is more reliable when the levels of three parameters vary. 

## Conclusion

Since our response variables, where highly linearly correlated, we came to the conclusion that the model which best fit the first moment would also be a good fit for the other models. We saw that after transforming Fr and Re into categorical variables, we greatly increased the Test MSE of our models. Taking the second central moment as an example, we saw some statistically significant interaction terms between Re and Fr, which, because of the hierarchy principle, means we included the order 1 terms Re_category and Fr_categrory as well. We predicted that the variance of the particle cluster would increase significantly if the gravitational acceleration and Reynold's number jointly decreased. When fitting a model only with the linear terms St, Re_category, and Fr_category, we saw that there was a statistically significant increase in the variance when the Reynold's number and gravitational acceleration decreased independently, but to a much lesser extent than with the interaction term.

In this study, we analyzed the effect of Re, Fr, and St to the distribution of particle cluster volumns. It is important to note some limitation of our results: First, we have a relatively small training dataset (<100). This might result in issues relating to generalization and data imbalance. Second, we only have three levels for Re and Fr. There will be a serious issue of bad extrapolation if we use this model to predict tuples with Re and Fr outside of these three levels. Therefore, this model is only reliable with these three levels of Re and Fr.

We believe that our model provides good prediction and on the probability distribution of particle cluster volumns, and interpretation on how Re, Fr, St affect the flows. With high Re, low St, low Fr, The turbulent flows' particles distribute in a more normal, symmetric way with few outliers.
When the flow is laminar, Re and Fr are both low, the probability distribution for particle cluster volumes is spread out, highly skewed, and is heavily influenced by outliers. When the flow is turbulent, the probability distribution for particle cluster volumes is fairly normal, symmetric, and with few outliers. The standard error of our results explode when the flow is turbulent (all three parameters are high), turbulent distributions are harder to predict. In future study, we hope to address this issue and explore how to produce more accurate prediction. 

## Citations

https://www.sciencedirect.com/topics/engineering/stokes-number

https://www.sciencedirect.com/topics/engineering/froude-number

https://www.sciencedirect.com/topics/engineering/reynolds-number

\newpage

## Appendix

### 1.1

The regression output for models on 1st raw moment and 2nd, 3rd, 4th central moment.
```{r}
summary(final_model_M1)$coefficients %>% kable(digits = 3)
summary(final_model_M2)$coefficients %>% kable(digits = 3)
summary(final_model_M3)$coefficients %>% kable(digits = 3)
summary(final_model_M4)$coefficients %>% kable(digits = 3)
```

### 1.2 Prediction output on the test data
```{r}
data_predicted_output %>% kable(digits = 5)
```

\newpage

### 1.3 SE Table (Uncertainty in Prediction)
```{r}
SE_M1 <- predict(final_model_M1, data_test_transformed, se.fit = TRUE)$se.fit
SE_M2 <- predict(final_model_M2, data_test_transformed, se.fit = TRUE)$se.fit
SE_M3 <- predict(final_model_M3, data_test_transformed, se.fit = TRUE)$se.fit
SE_M4 <- predict(final_model_M4, data_test_transformed, se.fit = TRUE)$se.fit

se.table = data.frame(SE_M1, SE_M2, SE_M3, SE_M4)

se.table%>%kable()
```

\newpage

### 1.4
### Model Diagnostics

```{r, echo=F} 
par(mfrow = c(2, 4))
plot(final_model_M1)
plot(final_model_M2)
plot(final_model_M3)
plot(final_model_M4)
```

We conducted model diagnostic on the final model from mainly four perspectives

- Residuals vs Fitted: M1 clearly demonstrates a horizontal line, with the residuals randomly scattered. For M2, M3, and M4 while the trace is not strictly a horizontal line, it is roughly horizontal with no egregiously distinct pattern of residual data points. Therefore, the linear relationship assumptions hold.

- Normal Q-Q: For M2, M3, and M4, the residual points follow the straight dashed line, with some slight deviations towards the lower theoretical quantiles. Therefore, the residuals are normally distributed. For M1, however, the residual points follow a straight line for the residuals in the middle quantiles, but deviate drastically towards the lower and upper tails. 

- Scale-Location: For M1, the points are equally spread which is a good indication of homoscedasticity. For M2, M3, and M4, however, some of the points are lumped together, while they are still scattered on a large scale. However, the red traceline does not seem horizontal which indicates a potential heteroscedasticity problem as the limitation of this model.

- Residuals vs Leverage: While there do not seem to be influential points according to Cook's distance. There do seem to be outliers with high leverage for each of the four moments M1, M2, M3 and M4 which as future work, we could investigate and conduct analysis on the underlying cause.